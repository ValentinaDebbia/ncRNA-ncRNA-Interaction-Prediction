{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP3VegGFMH1SQDOUdp0q0Vh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","import pickle\n","import math\n","import random\n","import gc\n","import matplotlib.colors as mcolors\n","from math import ceil\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import (accuracy_score, balanced_accuracy_score,\n","                             precision_recall_fscore_support, roc_auc_score,\n","                             average_precision_score, confusion_matrix,\n","                             precision_recall_curve, roc_curve)\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import matthews_corrcoef\n","from sklearn.preprocessing import StandardScaler\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader, random_split\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.colors import LinearSegmentedColormap"],"metadata":{"id":"1sJdMzG6C_ly","executionInfo":{"status":"ok","timestamp":1765265681296,"user_tz":-60,"elapsed":10213,"user":{"displayName":"Valentina Debbia","userId":"09021616900675101898"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["folds_dir = \"/content/folds\"\n","os.makedirs(folds_dir, exist_ok=True)\n","print(\"Directory created at:\", folds_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xk1KyVSYPlHV","executionInfo":{"status":"ok","timestamp":1765265681307,"user_tz":-60,"elapsed":7,"user":{"displayName":"Valentina Debbia","userId":"09021616900675101898"}},"outputId":"96c460c5-1c7b-4a29-cadd-804f7b6abe86"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Directory created at: /content/folds\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"7_PG49LvCthx","executionInfo":{"status":"ok","timestamp":1765265710471,"user_tz":-60,"elapsed":41,"user":{"displayName":"Valentina Debbia","userId":"09021616900675101898"}}},"outputs":[],"source":["# Paths of files coming from notebooks 1 and 2\n","dict_path = '/content/lncrna-pseudo_dictionary_pooled_embeddings_RNAFM.p'   # embeddings pickle file\n","folds_dir = f\"/content/folds\" # dataset file, divided in train/test"]},{"cell_type":"code","source":["# Cross-fold training script.\n","# MAX AND AVG POOLING\n","# ----------------------------\n","# Hyperparameters\n","# ----------------------------\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(seed)\n","\n","# Original hyperparameters (kept)\n","input_dimension = 2560  # final input expected (we construct from embeddings)\n","batch_size = 512\n","train_negative_reduction_factor = 1   # set to >1 to downsample negatives\n","validation_split_ratio = 0.2\n","epochs = 50\n","learning_rate = 0.0005\n","num_layers = 4\n","dropout = 0.2\n","hidden_dimension = 1024\n","warmup_epochs = 4\n","patience = 10\n","min_lr = 5e-5\n","\n","\n","out_models_dir = \"models_by_fold_lncrna_pseudo_denovo_concat\"\n","os.makedirs(out_models_dir, exist_ok=True)\n","save_metrics_path = \"cv_training_results_full_lncrna_pseudo_denovo_concat.pkl\"\n","save_plots_dir = \"cv_plots/lncrna_pseudo_denovo/concat_pool\"\n","os.makedirs(save_plots_dir, exist_ok=True)\n","\n","\n","# embeddings\n","if not os.path.exists(dict_path):\n","    raise FileNotFoundError(f\"Embeddings dict not found at {dict_path}\")\n","with open(dict_path, 'rb') as fh:\n","    embeddings_dict = pickle.load(fh)\n","print(\"Loaded embeddings dict with\", len(embeddings_dict), \"entries\")\n","\n","\n","# Colors / fonts (kept)\n","color2 = '#9999CC'\n","color3 = '#00BBD8'\n","color_green = '#88B04B'\n","color_red = '#FF765B'\n","font_ticks = 16\n","font_labels = 18\n","heat0 = color_green\n","heat2 = '#204900'\n","heat3 = '#102A00'\n","cv_folds_path = ''\n","\n","custom_cmap = LinearSegmentedColormap.from_list(\"soft_oranges\", [heat0, heat2, heat3])\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device:\", device)\n","\n","# ----------------------------\n","# Load folds and embeddings\n","# ----------------------------\n","if os.path.isdir(folds_dir):\n","    files = sorted([p for p in os.listdir(folds_dir) if p.endswith('.pkl')])\n","    folds = []\n","    for fn in files:\n","        with open(os.path.join(folds_dir, fn), 'rb') as fh:\n","            folds.append(pickle.load(fh))\n","    print(f\"Loaded {len(folds)} per-fold pickles from {folds_dir}\")\n","else:\n","    raise FileNotFoundError(f\"Neither {cv_folds_path} nor directory {folds_dir} found.\")\n","\n","\n","# Convert pair lists into numpy arrays (embeddings) and labels\n","def pairs_to_arrays(pairs_list, embeddings_dict, label):\n","    X_list = []\n","    y_list = []\n","    missing = 0\n","    for (s1, s2), (t1, t2) in pairs_list:\n","        if s1 not in embeddings_dict or s2 not in embeddings_dict:\n","            missing += 1\n","            continue\n","        e1 = np.asarray(embeddings_dict[s1], dtype=np.float32)\n","        e2 = np.asarray(embeddings_dict[s2], dtype=np.float32)\n","        # keep last 640 of each like original notebook\n","        e1s = e1\n","        e2s = e2\n","        if e1s.shape[0] < 1280 or e2s.shape[0] < 1280:\n","            # if shorter than 640, pad with zeros to 640 to keep shape consistent\n","            e1s = np.pad(e1s, (1280 - e1s.shape[0], 0), mode='constant') if e1s.shape[0] < 1280 else e1s\n","            e2s = np.pad(e2s, (1280 - e2s.shape[0], 0), mode='constant') if e2s.shape[0] < 1280 else e2s\n","        X_list.append(np.concatenate([e1s, e2s], axis=0))\n","        y_list.append(int(label))\n","    if missing:\n","        print(f\"  Skipped {missing} pairs due to missing embeddings.\")\n","    if len(X_list) == 0:\n","        return None, None\n","    X = np.stack(X_list)\n","    y = np.array(y_list, dtype=np.int64)\n","    return X, y\n","\n","\n","\n","\n","# Balanced batch sampler (same logic as notebook)\n","def get_data_loaders_without_replacement_from_arrays(X_train, y_train, X_valid, y_valid, batch_size, neg_batch_ratio=0.7, num_workers=2):\n","    \"\"\"\n","    Build DataLoaders using BalancedBatchSampler logic but from numpy arrays.\n","    \"\"\"\n","    train_tensor = torch.from_numpy(X_train).float()\n","    train_labels = torch.from_numpy(y_train)\n","    train_dataset = TensorDataset(train_tensor, train_labels)\n","\n","    valid_tensor = torch.from_numpy(X_valid).float()\n","    valid_labels = torch.from_numpy(y_valid)\n","    valid_dataset = TensorDataset(valid_tensor, valid_labels)\n","\n","    class BalancedBatchSampler:\n","        def __init__(self, labels, batch_size, neg_batch_ratio):\n","            self.labels = labels.numpy() if torch.is_tensor(labels) else labels\n","            self.batch_size = batch_size\n","            self.neg_batch_ratio = neg_batch_ratio\n","            self.neg_indices = np.where(self.labels == 0)[0]\n","            self.pos_indices = np.where(self.labels == 1)[0]\n","            self.num_neg_per_batch = int(batch_size * neg_batch_ratio)\n","            self.num_batches = len(self.neg_indices) // max(1, self.num_neg_per_batch)\n","            self.leftover_negatives = len(self.neg_indices) % max(1, self.num_neg_per_batch)\n","\n","        def __iter__(self):\n","            neg_idx = np.random.permutation(self.neg_indices)\n","            pos_idx = self.pos_indices\n","            batch_start = 0\n","            for batch_num in range(self.num_batches):\n","                if batch_num < self.num_batches - 1 or self.leftover_negatives == 0:\n","                    num_neg_in_batch = self.num_neg_per_batch\n","                else:\n","                    num_neg_in_batch = self.leftover_negatives\n","                neg_batch = neg_idx[batch_start:batch_start + num_neg_in_batch]\n","                batch_start += num_neg_in_batch\n","                num_pos_in_batch = self.batch_size - num_neg_in_batch\n","                pos_batch = np.random.choice(pos_idx, num_pos_in_batch, replace=True)\n","                batch_indices = np.concatenate([neg_batch, pos_batch])\n","                np.random.shuffle(batch_indices)\n","                yield batch_indices.tolist()\n","\n","        def __len__(self):\n","            return self.num_batches if self.num_batches > 0 else 1\n","\n","    train_sampler = BalancedBatchSampler(train_labels, batch_size, neg_batch_ratio=0.7)\n","    train_loader = DataLoader(train_dataset, batch_sampler=train_sampler, num_workers=4)\n","    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    return train_loader, valid_loader\n","\n","# InteractionNN (kept same)\n","class InteractionNN(nn.Module):\n","    def __init__(self, input_dim=input_dimension, hidden_dim=hidden_dimension, num_layers=num_layers, dropout=dropout):\n","        super(InteractionNN, self).__init__()\n","        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(p=dropout)]\n","        for _ in range(num_layers - 1):\n","            layers.append(nn.Linear(hidden_dim, hidden_dim))\n","            layers.append(nn.ReLU())\n","            layers.append(nn.Dropout(p=dropout))\n","        self.hidden_layers = nn.Sequential(*layers)\n","        self.output_layer = nn.Linear(hidden_dim, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.hidden_layers(x)\n","        x = self.output_layer(x)\n","        return torch.sigmoid(x).view(-1)\n","\n","# learning rate scheduler with warmup\n","def cosine_annealing_with_warmup(epoch, warmup_epochs, max_epochs, min_lr=1e-5):\n","    if epoch < warmup_epochs:\n","        return (epoch + 1) / warmup_epochs\n","    else:\n","        cosine_decay = 0.5 * (1 + math.cos(math.pi * (epoch - warmup_epochs) / (max_epochs - warmup_epochs)))\n","        return (cosine_decay * (1 - min_lr) + min_lr)\n","\n","# ----------------------------\n","# Cross-fold training\n","# ----------------------------\n","\n","# collect confusion matrices across folds\n","cms = []\n","all_test_probs_all = []\n","all_test_labels_all = []\n","roc_curves = []\n","pr_curves = []\n","\n","all_fold_metrics = []\n","for fi, fold in enumerate(folds):\n","    print(\"\\n=== Fold\", fi, \"===\")\n","    train_pos = fold['train']['positives']\n","    train_neg = fold['train']['negatives']\n","    test_pos = fold['test']['positives']\n","    test_neg = fold['test']['negatives']\n","\n","    # Build full training arrays from pairs\n","    Xp, yp = pairs_to_arrays(train_pos, embeddings_dict, label=1)\n","    Xn, yn = pairs_to_arrays(train_neg, embeddings_dict, label=0)\n","\n","    if Xp is None and Xn is None:\n","        print(\"No training data for fold\", fi); continue\n","    if Xp is None:\n","        X_all = Xn; y_all = yn\n","    elif Xn is None:\n","        X_all = Xp; y_all = yp\n","    else:\n","        X_all = np.concatenate([Xp, Xn], axis=0)\n","        y_all = np.concatenate([yp, yn], axis=0)\n","\n","    # Optionally reduce negatives like original script\n","    # Separate positives and negatives to apply reduction factor conveniently\n","    pos_idx = np.where(y_all == 1)[0]\n","    neg_idx = np.where(y_all == 0)[0]\n","    # Shuffle negatives\n","    np.random.shuffle(neg_idx)\n","    reduced_neg_count = max(1, int(len(neg_idx) // train_negative_reduction_factor))\n","    neg_selected = neg_idx[:reduced_neg_count]\n","    selected_idx = np.concatenate([pos_idx, neg_selected])\n","    np.random.shuffle(selected_idx)\n","    X_sel = X_all[selected_idx]\n","    y_sel = y_all[selected_idx]\n","\n","    print(f\" Train samples after reduction: {len(y_sel)} (pos={sum(y_sel==1)}, neg={sum(y_sel==0)})\")\n","\n","    # split into train/val\n","    n_total = len(y_sel)\n","    n_train = int((1 - validation_split_ratio) * n_total)\n","    n_val = n_total - n_train\n","    indices = np.arange(n_total)\n","    np.random.shuffle(indices)\n","    train_inds = indices[:n_train]\n","    val_inds = indices[n_train:]\n","\n","    X_train = X_sel[train_inds]; y_train = y_sel[train_inds]\n","    X_val = X_sel[val_inds]; y_val = y_sel[val_inds]\n","\n","    # Standardize per fold using training data\n","    scaler = StandardScaler()\n","    X_train = scaler.fit_transform(X_train)\n","    X_val = scaler.transform(X_val)\n","\n","    # Prepare test set arrays\n","    Xp_test, yp_test = pairs_to_arrays(test_pos, embeddings_dict, label=1)\n","    Xn_test, yn_test = pairs_to_arrays(test_neg, embeddings_dict, label=0)\n","    if Xp_test is None and Xn_test is None:\n","        print(\"No test data for fold\", fi); continue\n","    if Xp_test is None:\n","        X_test = Xn_test; y_test = yn_test\n","    elif Xn_test is None:\n","        X_test = Xp_test; y_test = yp_test\n","    else:\n","        X_test = np.concatenate([Xp_test, Xn_test], axis=0)\n","        y_test = np.concatenate([yp_test, yn_test], axis=0)\n","    X_test = scaler.transform(X_test)\n","\n","    # Build DataLoaders using balanced sampler approach\n","    train_loader, val_loader = get_data_loaders_without_replacement_from_arrays(X_train, y_train, X_val, y_val, batch_size, neg_batch_ratio=0.5)\n","\n","    test_ds = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test))\n","    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","    # model, optimizer\n","    model = InteractionNN(input_dim=input_dimension, hidden_dim=hidden_dimension, num_layers=num_layers, dropout=dropout).to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda e: cosine_annealing_with_warmup(e, warmup_epochs, epochs, min_lr))\n","    criterion = nn.BCELoss()\n","\n","    # training loop with early stopping on validation loss\n","    best_val_loss = float('inf')\n","    best_state = None\n","    epochs_no_improv = 0\n","\n","    train_losses = []\n","    val_losses = []\n","    for epoch in range(epochs):\n","        model.train()\n","        running_loss = 0.0\n","        for batch in train_loader:\n","            xb, yb = batch\n","            xb = xb.to(device)\n","            yb = yb.to(device, dtype=torch.float32)\n","            optimizer.zero_grad()\n","            outputs = model(xb).view(-1)\n","            loss = criterion(outputs, yb)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","        train_loss = running_loss / (len(train_loader) if len(train_loader)>0 else 1)\n","        train_losses.append(train_loss)\n","\n","        # validation\n","        model.eval()\n","        total_val_loss = 0.0\n","        all_val_probs = []\n","        all_val_labels = []\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                xb, yb = batch\n","                xb = xb.to(device)\n","                yb = yb.to(device, dtype=torch.float32)\n","                outputs = model(xb).view(-1)\n","                loss = criterion(outputs, yb)\n","                total_val_loss += loss.item()\n","                all_val_probs.extend(outputs.cpu().numpy())\n","                all_val_labels.extend(yb.cpu().numpy())\n","        val_loss = total_val_loss / (len(val_loader) if len(val_loader)>0 else 1)\n","        val_losses.append(val_loss)\n","\n","        # scheduler step & early stop logic\n","        if val_loss < best_val_loss - 1e-8:\n","            best_val_loss = val_loss\n","            best_state = model.state_dict()\n","            epochs_no_improv = 0\n","            # save best per-fold\n","            per_model_path = os.path.join(out_models_dir, f\"best_model_fold{fi}.pth\")\n","            torch.save(best_state, per_model_path)\n","        else:\n","            epochs_no_improv += 1\n","\n","        scheduler.step()\n","        lr_now = optimizer.param_groups[0]['lr']\n","        print(f\"Fold {fi} Epoch {epoch+1}/{epochs} TrainLoss={train_loss:.6f} ValLoss={val_loss:.6f} LR={lr_now:.2e}\")\n","\n","        if epochs_no_improv >= patience:\n","            print(f\"Early stopping on fold {fi} at epoch {epoch+1}\")\n","            break\n","\n","    # restore best state\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","\n","    # Compute optimal threshold on validation (maximize MCC, same as your notebook)\n","    model.eval()\n","    val_probs = np.array(all_val_probs)\n","    val_labels = np.array(all_val_labels)\n","    thresholds = np.linspace(0, 1, 101)\n","    mcc_scores = []\n","    for thr in thresholds:\n","        preds_thr = (val_probs > thr).astype(np.int32)\n","        mcc_scores.append(matthews_corrcoef(val_labels, preds_thr))\n","    best_thr = thresholds[int(np.nanargmax(mcc_scores))]\n","    print(\" Fold\", fi, \" optimal threshold (val MCC):\", best_thr)\n","\n","    # Evaluate on test set using best_thr\n","    all_test_probs = []\n","    all_test_labels = []\n","    with torch.no_grad():\n","        for xb, yb in test_loader:\n","            xb = xb.to(device)\n","            outputs = model(xb).view(-1)\n","            all_test_probs.extend(outputs.cpu().numpy())\n","            all_test_labels.extend(yb.numpy())\n","    all_test_probs = np.array(all_test_probs)\n","    all_test_labels = np.array(all_test_labels)\n","    test_preds = (all_test_probs > best_thr).astype(int)\n","\n","    # compute metrics on test\n","    acc = accuracy_score(all_test_labels, test_preds)\n","    bal_acc = balanced_accuracy_score(all_test_labels, test_preds)\n","    prec, rec, f1, _ = precision_recall_fscore_support(all_test_labels, test_preds, average='binary', zero_division=0)\n","    try:\n","        auroc = roc_auc_score(all_test_labels, all_test_probs)\n","    except Exception:\n","        auroc = float('nan')\n","    try:\n","        auprc = average_precision_score(all_test_labels, all_test_probs)\n","    except Exception:\n","        auprc = float('nan')\n","\n","    fold_metrics = {\n","        'fold': fi,\n","        'n_train': len(train_loader.dataset),\n","        'n_val': len(val_loader.dataset),\n","        'n_test': len(test_loader.dataset),\n","        'threshold': float(best_thr),\n","        'accuracy': float(acc),\n","        'balanced_accuracy': float(bal_acc),\n","        'precision': float(prec),\n","        'recall': float(rec),\n","        'f1': float(f1),\n","        'auroc': float(auroc),\n","        'auprc': float(auprc)\n","    }\n","    all_fold_metrics.append(fold_metrics)\n","    print(\" Fold\", fi, \"metrics:\", fold_metrics)\n","\n","    # X boxplots and confusion matrices\n","    cm = confusion_matrix(all_test_labels, test_preds, normalize=\"true\")\n","    cms.append(cm)\n","    all_test_probs_all.extend(all_test_probs)\n","    all_test_labels_all.extend(all_test_labels)\n","\n","    try:\n","        fpr, tpr, _ = roc_curve(all_test_labels, all_test_probs)\n","        roc_curves.append((fi, fpr, tpr, auroc))\n","    except Exception:\n","        pass\n","\n","    try:\n","        prec_vals, rec_vals, _ = precision_recall_curve(all_test_labels, all_test_probs)\n","        pr_curves.append((fi, rec_vals, prec_vals, auprc))\n","    except Exception:\n","        pass\n","\n","    # clear memory\n","    del model, optimizer, scheduler, train_loader, val_loader, test_loader\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","# ----------------------------\n","# Aggregate metrics across folds\n","# ----------------------------\n","df_res = pd.DataFrame(all_fold_metrics)\n","print(\"\\nPer-fold results:\")\n","display(df_res)\n","\n","agg = df_res.drop(columns=['fold','n_train','n_val','n_test','threshold']).agg(['mean','std']).T\n","agg['mean'] = agg['mean'].round(4)\n","agg['std'] = agg['std'].round(4)\n","print(\"\\nAggregated metrics (mean ± std):\")\n","display(agg)\n","\n","# =========================\n","# Plot metrics across folds\n","# =========================\n","colors = plt.cm.tab10.colors  # distinct colors\n","from matplotlib.backends.backend_pdf import PdfPages\n","metrics_to_plot = [\"accuracy\", \"balanced_accuracy\", \"precision\", \"recall\", \"f1\"]\n","# Custom display names\n","metric_display_names = {\n","    \"accuracy\": \"ACCURACY\",\n","    \"balanced_accuracy\": \"BAL. ACC.\",\n","    \"precision\": \"PRECISION\",\n","    \"recall\": \"RECALL\",\n","    \"f1\": \"F1\"\n","}\n","\n","pdf_path = os.path.join(save_plots_dir, \"metrics_summary_table.pdf\")\n","with PdfPages(pdf_path) as pdf:\n","    plt.figure(figsize=(8, 4))\n","    ax = plt.gca()\n","    ax.axis('off')\n","\n","    # Header row with display names\n","    header = [\"Fold\"] + [metric_display_names[m] for m in metrics_to_plot]\n","    table_data = [header]\n","\n","    for _, row in df_res.iterrows():\n","        table_data.append(\n","            [int(row[\"fold\"])] + [f\"{row[m]:.3f}\" for m in metrics_to_plot]\n","        )\n","\n","    # Mean ± Std row\n","    mean_std_vals = [f\"{df_res[m].mean():.3f} ± {df_res[m].std():.3f}\" for m in metrics_to_plot]\n","    table_data.append([\"Mean ± Std\"] + mean_std_vals)\n","\n","    # Draw table\n","    table = ax.table(cellText=table_data, loc=\"center\", cellLoc=\"center\")\n","    table.auto_set_font_size(False)\n","    table.set_fontsize(10)\n","    table.scale(1.2, 1.2)\n","\n","    pdf.savefig()\n","    plt.close()\n","\n","\n","# boxplot and normalised confusion matrix averaged across folds\n","cms = np.array(cms)\n","cm_mean = cms.mean(axis=0)\n","cm_std = cms.std(axis=0)\n","\n","plt.figure(figsize=(6, 5))\n","ax = sns.heatmap(cm_mean, annot=True, fmt=\".2%\", cmap=custom_cmap,\n","                 xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'],\n","                 annot_kws={\"size\": font_ticks}, cbar_kws={\"shrink\": 0.9})\n","\n","plt.xlabel('Predicted', fontsize=font_labels)\n","plt.ylabel('Actual', fontsize=font_labels)\n","plt.xticks(fontsize=font_ticks)\n","plt.yticks(fontsize=font_ticks)\n","\n","cbar = ax.collections[0].colorbar\n","cbar.ax.yaxis.set_tick_params(labelsize=font_ticks)\n","\n","plt.savefig(os.path.join(save_plots_dir, \"confusion_matrix_normalized_mean_std.pdf\"), bbox_inches=\"tight\")\n","plt.close()\n","\n","# boxoplot now\n","all_test_probs_all = np.array(all_test_probs_all)\n","all_test_labels_all = np.array(all_test_labels_all)\n","\n","plt.figure(figsize=(8, 6))\n","box = plt.boxplot([all_test_probs_all[all_test_labels_all == 0],\n","                   all_test_probs_all[all_test_labels_all == 1]],\n","                  labels=[\"Negative\", \"Positive\"], patch_artist=True)\n","\n","# Set colors: red for negatives, green for positives\n","box['boxes'][0].set(facecolor=color_red)   # Negative\n","box['boxes'][1].set(facecolor=color_green) # Positive\n","\n","# Median lines\n","for median in box['medians']:\n","    median.set(color='#FFD260', linewidth=2)\n","\n","plt.ylabel(\"Predicted Probability\", fontsize=font_labels)\n","plt.xticks(fontsize=font_ticks)\n","plt.yticks(fontsize=font_ticks)\n","plt.grid()\n","\n","plt.savefig(os.path.join(save_plots_dir, \"boxplot_probabilities_all_folds.pdf\"), bbox_inches=\"tight\")\n","plt.close()\n","\n","# =========================\n","# Plot AUROC curves all folds\n","# =========================\n","plt.figure(figsize=(6, 5))\n","for fi, fpr, tpr, auroc in roc_curves:\n","    plt.plot(fpr, tpr, color=colors[fi % len(colors)],\n","             label=f\"AUROC={auroc:.3f}\")\n","plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n","plt.xlabel('False Positive Rate', fontsize=font_labels)\n","plt.ylabel('True Positive Rate', fontsize=font_labels)\n","plt.grid(True)\n","plt.legend(fontsize=12, loc=\"lower right\")  # bigger legend inside, bottom-right\n","plt.tight_layout()\n","plt.savefig(os.path.join(save_plots_dir, \"all_folds_roc_curves.pdf\"), bbox_inches=\"tight\")\n","plt.close()\n","\n","# =========================\n","# Plot AUPRC curves all folds\n","# =========================\n","plt.figure(figsize=(6, 5))\n","for fi, rec_vals, prec_vals, auprc in pr_curves:\n","    plt.plot(rec_vals, prec_vals, color=colors[fi % len(colors)],\n","             label=f\"AUPRC={auprc:.3f}\")\n","plt.xlabel('Recall', fontsize=font_labels)\n","plt.ylabel('Precision', fontsize=font_labels)\n","plt.grid(True)\n","plt.legend(fontsize=12, loc=\"lower left\")  # bigger legend inside, bottom-left\n","plt.tight_layout()\n","plt.savefig(os.path.join(save_plots_dir, \"all_folds_pr_curves.pdf\"), bbox_inches=\"tight\")\n","plt.close()\n","\n","# Save metrics and per-fold models metadata\n","with open(save_metrics_path, 'wb') as fh:\n","    pickle.dump({'per_fold': all_fold_metrics, 'aggregate': agg.to_dict()}, fh)\n","print(\"Saved metrics to\", save_metrics_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"1Mx5bbaIDG49","executionInfo":{"status":"ok","timestamp":1765265736711,"user_tz":-60,"elapsed":24242,"user":{"displayName":"Valentina Debbia","userId":"09021616900675101898"}},"outputId":"698dc4f1-8b68-4c46-e614-662c644b6214"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded embeddings dict with 262 entries\n","Device: cuda\n","Loaded 1 per-fold pickles from /content/folds\n","\n","=== Fold 0 ===\n"," Train samples after reduction: 19086 (pos=3472, neg=15614)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Fold 0 Epoch 1/50 TrainLoss=0.343408 ValLoss=0.137649 LR=2.50e-04\n","Fold 0 Epoch 2/50 TrainLoss=0.144254 ValLoss=0.112628 LR=3.75e-04\n","Fold 0 Epoch 3/50 TrainLoss=0.116640 ValLoss=0.132385 LR=5.00e-04\n","Fold 0 Epoch 4/50 TrainLoss=0.103295 ValLoss=0.118541 LR=5.00e-04\n","Fold 0 Epoch 5/50 TrainLoss=0.091342 ValLoss=0.105212 LR=4.99e-04\n","Fold 0 Epoch 6/50 TrainLoss=0.072733 ValLoss=0.149574 LR=4.98e-04\n","Fold 0 Epoch 7/50 TrainLoss=0.062397 ValLoss=0.097462 LR=4.95e-04\n","Fold 0 Epoch 8/50 TrainLoss=0.052935 ValLoss=0.103418 LR=4.91e-04\n","Fold 0 Epoch 9/50 TrainLoss=0.048461 ValLoss=0.112553 LR=4.86e-04\n","Fold 0 Epoch 10/50 TrainLoss=0.042739 ValLoss=0.112111 LR=4.79e-04\n","Fold 0 Epoch 11/50 TrainLoss=0.036200 ValLoss=0.121387 LR=4.72e-04\n","Fold 0 Epoch 12/50 TrainLoss=0.039029 ValLoss=0.143365 LR=4.64e-04\n","Fold 0 Epoch 13/50 TrainLoss=0.034459 ValLoss=0.113704 LR=4.54e-04\n","Fold 0 Epoch 14/50 TrainLoss=0.028340 ValLoss=0.144261 LR=4.44e-04\n","Fold 0 Epoch 15/50 TrainLoss=0.031335 ValLoss=0.148082 LR=4.33e-04\n","Fold 0 Epoch 16/50 TrainLoss=0.019207 ValLoss=0.140654 LR=4.21e-04\n","Fold 0 Epoch 17/50 TrainLoss=0.015773 ValLoss=0.163545 LR=4.08e-04\n","Early stopping on fold 0 at epoch 17\n"," Fold 0  optimal threshold (val MCC): 0.96\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Fold 0 metrics: {'fold': 0, 'n_train': 15268, 'n_val': 3818, 'n_test': 6205, 'threshold': 0.96, 'accuracy': 0.9911361804995971, 'balanced_accuracy': 0.9765179440286876, 'precision': 0.9799291617473436, 'recall': 0.956221198156682, 'f1': 0.967930029154519, 'auroc': 0.9964997422566916, 'auprc': 0.9868173280095116}\n","\n","Per-fold results:\n"]},{"output_type":"display_data","data":{"text/plain":["   fold  n_train  n_val  n_test  threshold  accuracy  balanced_accuracy  \\\n","0     0    15268   3818    6205       0.96  0.991136           0.976518   \n","\n","   precision    recall       f1   auroc     auprc  \n","0   0.979929  0.956221  0.96793  0.9965  0.986817  "],"text/html":["\n","  <div id=\"df-4e60097c-1367-4156-ae1c-7b499a708c0e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fold</th>\n","      <th>n_train</th>\n","      <th>n_val</th>\n","      <th>n_test</th>\n","      <th>threshold</th>\n","      <th>accuracy</th>\n","      <th>balanced_accuracy</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>f1</th>\n","      <th>auroc</th>\n","      <th>auprc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>15268</td>\n","      <td>3818</td>\n","      <td>6205</td>\n","      <td>0.96</td>\n","      <td>0.991136</td>\n","      <td>0.976518</td>\n","      <td>0.979929</td>\n","      <td>0.956221</td>\n","      <td>0.96793</td>\n","      <td>0.9965</td>\n","      <td>0.986817</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e60097c-1367-4156-ae1c-7b499a708c0e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-4e60097c-1367-4156-ae1c-7b499a708c0e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-4e60097c-1367-4156-ae1c-7b499a708c0e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_9f1ed71e-7884-4a0d-85a2-52d8dd509893\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_res')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_9f1ed71e-7884-4a0d-85a2-52d8dd509893 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_res');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_res","summary":"{\n  \"name\": \"df_res\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"fold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_train\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 15268,\n        \"max\": 15268,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          15268\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_val\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 3818,\n        \"max\": 3818,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3818\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_test\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 6205,\n        \"max\": 6205,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          6205\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"threshold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.96,\n        \"max\": 0.96,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.96\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9911361804995971,\n        \"max\": 0.9911361804995971,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9911361804995971\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"balanced_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9765179440286876,\n        \"max\": 0.9765179440286876,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9765179440286876\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9799291617473436,\n        \"max\": 0.9799291617473436,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9799291617473436\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.956221198156682,\n        \"max\": 0.956221198156682,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.956221198156682\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.967930029154519,\n        \"max\": 0.967930029154519,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.967930029154519\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"auroc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9964997422566916,\n        \"max\": 0.9964997422566916,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9964997422566916\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"auprc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9868173280095116,\n        \"max\": 0.9868173280095116,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9868173280095116\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Aggregated metrics (mean ± std):\n"]},{"output_type":"display_data","data":{"text/plain":["                     mean  std\n","accuracy           0.9911  NaN\n","balanced_accuracy  0.9765  NaN\n","precision          0.9799  NaN\n","recall             0.9562  NaN\n","f1                 0.9679  NaN\n","auroc              0.9965  NaN\n","auprc              0.9868  NaN"],"text/html":["\n","  <div id=\"df-6ddad357-dd17-46d8-bea4-22680c4b1d65\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean</th>\n","      <th>std</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>accuracy</th>\n","      <td>0.9911</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>balanced_accuracy</th>\n","      <td>0.9765</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>precision</th>\n","      <td>0.9799</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>recall</th>\n","      <td>0.9562</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>f1</th>\n","      <td>0.9679</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>auroc</th>\n","      <td>0.9965</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>auprc</th>\n","      <td>0.9868</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ddad357-dd17-46d8-bea4-22680c4b1d65')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-6ddad357-dd17-46d8-bea4-22680c4b1d65 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-6ddad357-dd17-46d8-bea4-22680c4b1d65');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-0f8d406e-73a3-40e9-868f-4e5ae4f24268\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0f8d406e-73a3-40e9-868f-4e5ae4f24268')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-0f8d406e-73a3-40e9-868f-4e5ae4f24268 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_94ae823a-3562-4666-822d-e0114cee3385\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('agg')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_94ae823a-3562-4666-822d-e0114cee3385 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('agg');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"agg","summary":"{\n  \"name\": \"agg\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.013922968347986987,\n        \"min\": 0.9562,\n        \"max\": 0.9965,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.9911,\n          0.9765,\n          0.9965\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2205183482.py:496: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  box = plt.boxplot([all_test_probs_all[all_test_labels_all == 0],\n"]},{"output_type":"stream","name":"stdout","text":["Saved metrics to cv_training_results_full_lncrna_pseudo_denovo_concat.pkl\n"]}]},{"cell_type":"code","source":["import os\n","import pickle\n","import math\n","import random\n","import gc\n","from math import ceil\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import (accuracy_score, balanced_accuracy_score,\n","                             precision_recall_fscore_support, roc_auc_score,\n","                             average_precision_score, confusion_matrix,\n","                             precision_recall_curve, roc_curve)\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import matthews_corrcoef\n","from sklearn.preprocessing import StandardScaler\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader, random_split\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.colors import LinearSegmentedColormap\n","\n","# ==========================================\n","# HYPERPARAMETER CONFIGURATIONS TO TEST\n","# ==========================================\n","\n","hyperparameter_configs = [\n","    {\n","        'name': 'Baseline',\n","        'learning_rate': 0.0005,\n","        'num_layers': 4,\n","        'dropout': 0.2,\n","        'hidden_dimension': 1024,\n","        'batch_size': 512,\n","        'warmup_epochs': 4,\n","    },\n","    {\n","        'name': 'Deeper_HighDropout',\n","        'learning_rate': 0.0003,\n","        'num_layers': 6,\n","        'dropout': 0.3,\n","        'hidden_dimension': 1024,\n","        'batch_size': 512,\n","        'warmup_epochs': 5,\n","    },\n","    {\n","        'name': 'Wider_LowLR',\n","        'learning_rate': 0.0001,\n","        'num_layers': 4,\n","        'dropout': 0.2,\n","        'hidden_dimension': 2048,\n","        'batch_size': 512,\n","        'warmup_epochs': 6,\n","    },\n","    {\n","        'name': 'Smaller_HighLR',\n","        'learning_rate': 0.001,\n","        'num_layers': 3,\n","        'dropout': 0.15,\n","        'hidden_dimension': 512,\n","        'batch_size': 256,\n","        'warmup_epochs': 3,\n","    },\n","    {\n","        'name': 'Balanced',\n","        'learning_rate': 0.0002,\n","        'num_layers': 5,\n","        'dropout': 0.25,\n","        'hidden_dimension': 1536,\n","        'batch_size': 384,\n","        'warmup_epochs': 5,\n","    },\n","]\n","\n","seed = 42\n","input_dimension = 2560\n","epochs = 50\n","min_lr = 5e-5\n","patience = 10\n","validation_split_ratio = 0.2\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(seed)\n","\n","color2 = '#9999CC'\n","color3 = '#00BBD8'\n","color_green = '#88B04B'\n","color_red = '#FF765B'\n","font_ticks = 16\n","font_labels = 18\n","heat0 = color_green\n","heat2 = '#204900'\n","heat3 = '#102A00'\n","\n","custom_cmap = LinearSegmentedColormap.from_list(\"soft_oranges\", [heat0, heat2, heat3])\n","\n","blue_cmap = LinearSegmentedColormap.from_list(\n","    \"soft_blues\",\n","    [\"#dceeff\", \"#6aaed6\", \"#1b6ca8\", \"#08306b\"]\n",")\n","\n","class InteractionNN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n","        super(InteractionNN, self).__init__()\n","        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(p=dropout)]\n","        for _ in range(num_layers - 1):\n","            layers.append(nn.Linear(hidden_dim, hidden_dim))\n","            layers.append(nn.ReLU())\n","            layers.append(nn.Dropout(p=dropout))\n","        self.hidden_layers = nn.Sequential(*layers)\n","        self.output_layer = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, x):\n","        x = self.hidden_layers(x)\n","        x = self.output_layer(x)\n","        return torch.sigmoid(x).view(-1)\n","\n","def cosine_annealing_with_warmup(epoch, warmup_epochs, max_epochs, min_lr=1e-5):\n","    if epoch < warmup_epochs:\n","        return (epoch + 1) / warmup_epochs\n","    else:\n","        cosine_decay = 0.5 * (1 + math.cos(math.pi * (epoch - warmup_epochs) / (max_epochs - warmup_epochs)))\n","        return (cosine_decay * (1 - min_lr) + min_lr)\n","\n","class BalancedBatchSampler:\n","    def __init__(self, labels, batch_size, neg_batch_ratio=0.7):\n","        self.labels = labels.numpy() if torch.is_tensor(labels) else labels\n","        self.batch_size = batch_size\n","        self.neg_batch_ratio = neg_batch_ratio\n","        self.neg_indices = np.where(self.labels == 0)[0]\n","        self.pos_indices = np.where(self.labels == 1)[0]\n","        self.num_neg_per_batch = int(batch_size * neg_batch_ratio)\n","        self.num_batches = len(self.neg_indices) // max(1, self.num_neg_per_batch)\n","\n","    def __iter__(self):\n","        neg_idx = np.random.permutation(self.neg_indices)\n","        pos_idx = self.pos_indices\n","        batch_start = 0\n","        for batch_num in range(self.num_batches):\n","            num_neg_in_batch = self.num_neg_per_batch\n","            neg_batch = neg_idx[batch_start:batch_start + num_neg_in_batch]\n","            batch_start += num_neg_in_batch\n","            num_pos_in_batch = self.batch_size - num_neg_in_batch\n","            pos_batch = np.random.choice(pos_idx, num_pos_in_batch, replace=True)\n","            batch_indices = np.concatenate([neg_batch, pos_batch])\n","            np.random.shuffle(batch_indices)\n","            yield batch_indices.tolist()\n","\n","    def __len__(self):\n","        return self.num_batches if self.num_batches > 0 else 1\n","\n","def train_with_config(config, X_train, y_train, X_val, y_val, X_test, y_test, fold_id=0, output_dir='hyperparameter_tuning_results'):\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Training Config: {config['name']}\")\n","    print(f\"{'='*60}\")\n","\n","    config_plot_dir = os.path.join(output_dir, f\"plots_{config['name']}\")\n","    os.makedirs(config_plot_dir, exist_ok=True)\n","\n","    train_tensor = torch.from_numpy(X_train).float()\n","    train_labels = torch.from_numpy(y_train)\n","    train_dataset = TensorDataset(train_tensor, train_labels)\n","\n","    val_tensor = torch.from_numpy(X_val).float()\n","    val_labels = torch.from_numpy(y_val)\n","    val_dataset = TensorDataset(val_tensor, val_labels)\n","\n","    train_sampler = BalancedBatchSampler(train_labels, config['batch_size'], neg_batch_ratio=0.7)\n","    train_loader = DataLoader(train_dataset, batch_sampler=train_sampler, num_workers=2)\n","    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)\n","\n","    test_ds = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test))\n","    test_loader = DataLoader(test_ds, batch_size=config['batch_size'], shuffle=False, num_workers=2)\n","\n","    model = InteractionNN(\n","        input_dim=input_dimension,\n","        hidden_dim=config['hidden_dimension'],\n","        num_layers=config['num_layers'],\n","        dropout=config['dropout']\n","    ).to(device)\n","\n","    total_params = sum(p.numel() for p in model.parameters())\n","    print(f\"Model parameters: {total_params:,}\")\n","\n","    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n","    scheduler = torch.optim.lr_scheduler.LambdaLR(\n","        optimizer,\n","        lr_lambda=lambda e: cosine_annealing_with_warmup(e, config['warmup_epochs'], epochs, min_lr)\n","    )\n","    criterion = nn.BCELoss()\n","\n","    best_val_loss = float('inf')\n","    best_state = None\n","    epochs_no_improv = 0\n","\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        running_loss = 0.0\n","        for batch in train_loader:\n","            xb, yb = batch\n","            xb = xb.to(device)\n","            yb = yb.to(device, dtype=torch.float32)\n","            optimizer.zero_grad()\n","            outputs = model(xb).view(-1)\n","            loss = criterion(outputs, yb)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","        train_loss = running_loss / len(train_loader)\n","        train_losses.append(train_loss)\n","\n","        model.eval()\n","        total_val_loss = 0.0\n","        all_val_probs = []\n","        all_val_labels = []\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                xb, yb = batch\n","                xb = xb.to(device)\n","                yb = yb.to(device, dtype=torch.float32)\n","                outputs = model(xb).view(-1)\n","                loss = criterion(outputs, yb)\n","                total_val_loss += loss.item()\n","                all_val_probs.extend(outputs.cpu().numpy())\n","                all_val_labels.extend(yb.cpu().numpy())\n","        val_loss = total_val_loss / len(val_loader)\n","        val_losses.append(val_loss)\n","\n","        if val_loss < best_val_loss - 1e-8:\n","            best_val_loss = val_loss\n","            best_state = model.state_dict()\n","            epochs_no_improv = 0\n","        else:\n","            epochs_no_improv += 1\n","\n","        scheduler.step()\n","        lr_now = optimizer.param_groups[0]['lr']\n","\n","        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | LR: {lr_now:.2e}\")\n","\n","        if epochs_no_improv >= patience:\n","            print(f\"Early stopping at epoch {epoch+1}\")\n","            break\n","\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","\n","    model.eval()\n","    val_probs = np.array(all_val_probs)\n","    val_labels = np.array(all_val_labels)\n","    thresholds = np.linspace(0, 1, 101)\n","    f1_scores = []\n","    for thr in thresholds:\n","        preds_thr = (val_probs > thr).astype(np.int32)\n","        _, _, f1, _ = precision_recall_fscore_support(val_labels, preds_thr, average='binary', zero_division=0)\n","        f1_scores.append(f1)\n","    best_thr = thresholds[int(np.nanargmax(f1_scores))]\n","    print(f\"Optimal threshold (val F1): {best_thr:.3f}\")\n","\n","    all_test_probs = []\n","    all_test_labels = []\n","    with torch.no_grad():\n","        for xb, yb in test_loader:\n","            xb = xb.to(device)\n","            outputs = model(xb).view(-1)\n","            all_test_probs.extend(outputs.cpu().numpy())\n","            all_test_labels.extend(yb.numpy())\n","\n","    all_test_probs = np.array(all_test_probs)\n","    all_test_labels = np.array(all_test_labels)\n","    test_preds = (all_test_probs > best_thr).astype(int)\n","\n","    accuracy = accuracy_score(all_test_labels, test_preds)\n","    balanced_acc = balanced_accuracy_score(all_test_labels, test_preds)\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        all_test_labels, test_preds, average='binary', zero_division=0\n","    )\n","    auroc = roc_auc_score(all_test_labels, all_test_probs)\n","    auprc = average_precision_score(all_test_labels, all_test_probs)\n","\n","    fpr, tpr, _ = roc_curve(all_test_labels, all_test_probs)\n","    plt.figure(figsize=(6, 5))\n","    plt.plot(fpr, tpr, color=color_green, linewidth=2, label=f'AUROC={auroc:.3f}')\n","    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title(f'ROC Curve - {config[\"name\"]}')\n","    plt.xticks(fontsize=font_ticks)\n","    plt.yticks(fontsize=font_ticks)\n","    plt.grid(True, alpha=0.3)\n","    plt.legend(fontsize=14, loc=\"lower right\")\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(config_plot_dir, \"roc_curve.pdf\"), bbox_inches=\"tight\")\n","    plt.close()\n","\n","    prec_vals, rec_vals, _ = precision_recall_curve(all_test_labels, all_test_probs)\n","    plt.figure(figsize=(6, 5))\n","    plt.plot(rec_vals, prec_vals, color=color3, linewidth=2, label=f'AUPRC={auprc:.3f}')\n","    plt.xlabel('Recall')\n","    plt.ylabel('Precision')\n","    plt.title(f'Precision-Recall Curve - {config[\"name\"]}')\n","    plt.xticks(fontsize=font_ticks)\n","    plt.yticks(fontsize=font_ticks)\n","    plt.grid(True, alpha=0.3)\n","    plt.legend(fontsize=14, loc=\"lower left\")\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(config_plot_dir, \"pr_curve.pdf\"), bbox_inches=\"tight\")\n","    plt.close()\n","\n","    cm = confusion_matrix(all_test_labels, test_preds, normalize=\"true\")\n","    plt.figure(figsize=(6, 5))\n","\n","    ax = sns.heatmap(\n","        cm,\n","        annot=True,\n","        fmt=\".2%\",\n","        cmap=blue_cmap,\n","        xticklabels=['Negative', 'Positive'],\n","        yticklabels=['Negative', 'Positive'],\n","        annot_kws={\"size\": font_ticks},\n","        cbar_kws={\"shrink\": 0.9}\n","    )\n","\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.title(f'Confusion Matrix - {config[\"name\"]}')\n","    plt.xticks(fontsize=font_ticks)\n","    plt.yticks(fontsize=font_ticks)\n","    cbar = ax.collections[0].colorbar\n","    cbar.ax.yaxis.set_tick_params(labelsize=font_ticks)\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(config_plot_dir, \"confusion_matrix.pdf\"), bbox_inches=\"tight\")\n","    plt.close()\n","\n","    plt.figure(figsize=(8, 6))\n","    box = plt.boxplot([all_test_probs[all_test_labels == 0],\n","                       all_test_probs[all_test_labels == 1]],\n","                      labels=[\"Negative\", \"Positive\"], patch_artist=True)\n","    box['boxes'][0].set(facecolor=color_red)\n","    box['boxes'][1].set(facecolor=color_green)\n","\n","    for median in box['medians']:\n","        median.set(color='#FFD260', linewidth=2)\n","\n","    plt.ylabel(\"Predicted Probability\")\n","    plt.title(f'Predicted Probabilities - {config[\"name\"]}')\n","    plt.xticks(fontsize=font_ticks)\n","    plt.yticks(fontsize=font_ticks)\n","    plt.grid(axis='y', alpha=0.3)\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(config_plot_dir, \"boxplot_probabilities.pdf\"), bbox_inches=\"tight\")\n","    plt.close()\n","\n","    results = {\n","        'config_name': config['name'],\n","        'fold': fold_id,\n","        'accuracy': float(accuracy),\n","        'balanced_accuracy': float(balanced_acc),\n","        'precision': float(precision),\n","        'recall': float(recall),\n","        'f1': float(f1),\n","        'auroc': float(auroc),\n","        'auprc': float(auprc),\n","        'threshold': float(best_thr),\n","        'total_params': total_params,\n","        'train_losses': train_losses,\n","        'val_losses': val_losses,\n","        'test_probs': all_test_probs,\n","        'test_labels': all_test_labels,\n","        'test_preds': test_preds,\n","        **config\n","    }\n","\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Results for {config['name']}:\")\n","    print(f\"  Accuracy:          {accuracy:.4f}\")\n","    print(f\"  Balanced Accuracy: {balanced_acc:.4f}\")\n","    print(f\"  Precision:         {precision:.4f}\")\n","    print(f\"  Recall:            {recall:.4f}\")\n","    print(f\"  F1 Score:          {f1:.4f}\")\n","    print(f\"  AUROC:             {auroc:.4f}\")\n","    print(f\"  AUPRC:             {auprc:.4f}\")\n","    print(f\"  Plots saved to:    {config_plot_dir}\")\n","    print(f\"{'='*60}\\n\")\n","\n","    return results, model\n","\n","def run_hyperparameter_tuning(folds_dir, dict_path, output_dir='hyperparameter_tuning_results'):\n","    \"\"\"\n","    Main function to run hyperparameter tuning across all configurations.\n","    \"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Load embeddings\n","    with open(dict_path, 'rb') as fh:\n","        embeddings_dict = pickle.load(fh)\n","    print(f\"Loaded embeddings: {len(embeddings_dict)} sequences\")\n","\n","    # Load fold data\n","    fold_files = sorted([f for f in os.listdir(folds_dir) if f.endswith('.pkl')])\n","    if not fold_files:\n","        raise FileNotFoundError(f\"No fold files found in {folds_dir}\")\n","\n","    # Use first fold\n","    fold_path = os.path.join(folds_dir, fold_files[0])\n","    with open(fold_path, 'rb') as fh:\n","        fold_data = pickle.load(fh)\n","\n","    print(f\"Loaded fold data from {fold_path}\")\n","\n","    # Prepare data\n","    def pairs_to_arrays(pairs_list, embeddings_dict, label):\n","        X_list = []\n","        y_list = []\n","        missing = 0\n","        for (s1, s2), (t1, t2) in pairs_list:\n","            if s1 not in embeddings_dict or s2 not in embeddings_dict:\n","                missing += 1\n","                continue\n","            e1 = np.asarray(embeddings_dict[s1], dtype=np.float32)\n","            e2 = np.asarray(embeddings_dict[s2], dtype=np.float32)\n","            e1s = e1 if e1.shape[0] >= 1280 else np.pad(e1, (1280 - e1.shape[0], 0), mode='constant')\n","            e2s = e2 if e2.shape[0] >= 1280 else np.pad(e2, (1280 - e2.shape[0], 0), mode='constant')\n","            X_list.append(np.concatenate([e1s, e2s], axis=0))\n","            y_list.append(int(label))\n","        if missing:\n","            print(f\"  Skipped {missing} pairs due to missing embeddings.\")\n","        if len(X_list) == 0:\n","            return None, None\n","        return np.stack(X_list), np.array(y_list, dtype=np.int64)\n","\n","    # Extract train and test data\n","    train_pos = fold_data['train']['positives']\n","    train_neg = fold_data['train']['negatives']\n","    test_pos = fold_data['test']['positives']\n","    test_neg = fold_data['test']['negatives']\n","\n","    Xp, yp = pairs_to_arrays(train_pos, embeddings_dict, label=1)\n","    Xn, yn = pairs_to_arrays(train_neg, embeddings_dict, label=0)\n","    X_all = np.concatenate([Xp, Xn], axis=0)\n","    y_all = np.concatenate([yp, yn], axis=0)\n","\n","    # Train/val split\n","    n_total = len(y_all)\n","    n_train = int((1 - validation_split_ratio) * n_total)\n","    indices = np.arange(n_total)\n","    np.random.shuffle(indices)\n","    train_inds = indices[:n_train]\n","    val_inds = indices[n_train:]\n","\n","    X_train_raw = X_all[train_inds]\n","    y_train = y_all[train_inds]\n","    X_val_raw = X_all[val_inds]\n","    y_val = y_all[val_inds]\n","\n","    # Test data\n","    Xp_test, yp_test = pairs_to_arrays(test_pos, embeddings_dict, label=1)\n","    Xn_test, yn_test = pairs_to_arrays(test_neg, embeddings_dict, label=0)\n","    X_test_raw = np.concatenate([Xp_test, Xn_test], axis=0)\n","    y_test = np.concatenate([yp_test, yn_test], axis=0)\n","\n","    # Standardize\n","    scaler = StandardScaler()\n","    X_train = scaler.fit_transform(X_train_raw)\n","    X_val = scaler.transform(X_val_raw)\n","    X_test = scaler.transform(X_test_raw)\n","\n","    print(f\"\\nData shapes:\")\n","    print(f\"  Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n","\n","    # Train all configurations\n","    all_results = []\n","    for config in hyperparameter_configs:\n","        results, model = train_with_config(\n","            config, X_train, y_train, X_val, y_val, X_test, y_test, fold_id=0, output_dir=output_dir\n","        )\n","        all_results.append(results)\n","\n","        # Save model\n","        model_path = os.path.join(output_dir, f\"model_{config['name']}.pth\")\n","        torch.save(model.state_dict(), model_path)\n","\n","    # Save results\n","    results_df = pd.DataFrame(all_results)\n","    results_df.to_csv(os.path.join(output_dir, 'hyperparameter_results.csv'), index=False)\n","\n","    with open(os.path.join(output_dir, 'all_results.pkl'), 'wb') as f:\n","        pickle.dump(all_results, f)\n","\n","    # Generate comparison plots\n","    generate_comparison_plots(all_results, output_dir)\n","\n","    return results_df, all_results\n","\n","def generate_comparison_plots(all_results, output_dir):\n","    \"\"\"Generate comparison plots for all configurations.\"\"\"\n","\n","    for result in all_results:\n","        result['test_probs'] = np.array(result['test_probs'])\n","        result['test_labels'] = np.array(result['test_labels'])\n","        result['test_preds'] = np.array(result['test_preds'])\n","\n","    df = pd.DataFrame(all_results)\n","    config_order = ['Baseline', 'Deeper_HighDropout', 'Wider_LowLR', 'Smaller_HighLR', 'Balanced']\n","    df['config_name'] = pd.Categorical(df['config_name'], categories=config_order, ordered=True)\n","    df = df.sort_values('config_name')\n","\n","    # 1. Bar plot comparison with 7 metrics\n","    fig, axes = plt.subplots(4, 2, figsize=(16, 18))\n","    axes = axes.flatten()\n","\n","    metrics = ['accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1', 'auroc', 'auprc']\n","    titles = ['Accuracy', 'Balanced Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUROC', 'AUPRC']\n","\n","    for idx, (metric, title) in enumerate(zip(metrics, titles)):\n","        ax = axes[idx]\n","        bar_width = 0.45\n","        x = np.arange(len(df))\n","        bars = ax.bar(range(len(df)), df[metric])\n","\n","        for bar in bars:\n","          bar.set_color('#1f77b4')\n","\n","        # Color best bar\n","        best_idx = df[metric].idxmax()\n","        best_position = df.index.get_loc(best_idx)\n","        bars[best_position].set_color('#c16dff')\n","\n","        ax.set_xticks(range(len(df)))\n","        ax.set_xticklabels(df['config_name'])\n","        ax.set_ylabel(title)\n","        ax.set_title(f'{title} Comparison')\n","        ax.grid(axis='y', alpha=0.3)\n","\n","        for bar, val in zip(bars, df[metric]):\n","          height = bar.get_height()\n","          ax.text(\n","              bar.get_x() + bar.get_width() / 2.,\n","              height,\n","              f'{val:.3f}',\n","              ha='center',\n","              va='bottom',\n","              fontsize=9\n","              )\n","\n","    for idx in range(len(metrics), len(axes)):\n","        fig.delaxes(axes[idx])\n","\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(output_dir, 'metrics_comparison.pdf'), bbox_inches='tight')\n","    plt.close()\n","\n","    # 2. Summary table\n","    fig, ax = plt.subplots(figsize=(16, 6))\n","    ax.axis('tight')\n","    ax.axis('off')\n","\n","    table_data = []\n","    table_data.append(['Config', 'Acc', 'Bal Acc', 'Prec', 'Rec', 'F1', 'AUROC', 'AUPRC', 'Params', 'Hidden', 'Layers', 'LR'])\n","\n","    for _, row in df.iterrows():\n","        table_data.append([\n","            row['config_name'],\n","            f\"{row['accuracy']:.4f}\",\n","            f\"{row['balanced_accuracy']:.4f}\",\n","            f\"{row['precision']:.4f}\",\n","            f\"{row['recall']:.4f}\",\n","            f\"{row['f1']:.4f}\",\n","            f\"{row['auroc']:.4f}\",\n","            f\"{row['auprc']:.4f}\",\n","            f\"{row['total_params']/1e6:.2f}M\",\n","            str(row['hidden_dimension']),\n","            str(row['num_layers']),\n","            f\"{row['learning_rate']:.4f}\"\n","        ])\n","\n","    table = ax.table(cellText=table_data, loc='center', cellLoc='center')\n","    table.auto_set_font_size(False)\n","    table.set_fontsize(8)\n","    table.scale(1.2, 2)\n","\n","    # Highlight header\n","    for i in range(len(table_data[0])):\n","        table[(0, i)].set_facecolor('#88B04B')\n","        table[(0, i)].set_text_props(weight='bold', color='white')\n","\n","    plt.savefig(os.path.join(output_dir, 'results_table.pdf'), bbox_inches='tight')\n","    plt.close()\n","\n","    # 3. Combined ROC curves (all configs on one plot)\n","    plt.figure(figsize=(8, 6))\n","    colors_list = plt.cm.tab10.colors\n","    for idx, row in df.iterrows():\n","        fpr, tpr, _ = roc_curve(row['test_labels'], row['test_probs'])\n","        plt.plot(fpr, tpr, color=colors_list[idx % len(colors_list)], linewidth=2,\n","                label=f\"{row['config_name']} (AUROC={row['auroc']:.3f})\")\n","    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate', fontsize=font_labels)\n","    plt.title('ROC Curves - All Configurations')\n","    plt.xticks(fontsize=font_ticks)\n","    plt.yticks(fontsize=font_ticks)\n","    plt.grid(True, alpha=0.3)\n","    plt.legend(fontsize=10, loc=\"lower right\")\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(output_dir, 'combined_roc_curves.pdf'), bbox_inches=\"tight\")\n","    plt.close()\n","\n","    # 4. Combined PR curves (all configs on one plot)\n","    plt.figure(figsize=(8, 6))\n","    for idx, row in df.iterrows():\n","        prec_vals, rec_vals, _ = precision_recall_curve(row['test_labels'], row['test_probs'])\n","        plt.plot(rec_vals, prec_vals, color=colors_list[idx % len(colors_list)], linewidth=2,\n","                label=f\"{row['config_name']} (AUPRC={row['auprc']:.3f})\")\n","    plt.xlabel('Recall')\n","    plt.ylabel('Precision')\n","    plt.title('Precision-Recall Curves - All Configurations')\n","    plt.xticks(fontsize=font_ticks)\n","    plt.yticks(fontsize=font_ticks)\n","    plt.grid(True, alpha=0.3)\n","    plt.legend(fontsize=10, loc=\"lower left\")\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(output_dir, 'combined_pr_curves.pdf'), bbox_inches=\"tight\")\n","    plt.close()\n","\n","    print(f\"\\nPlots saved to {output_dir}\")\n","    print(f\"\\nBest configurations:\")\n","    print(f\"  Accuracy:          {df.loc[df['accuracy'].idxmax(), 'config_name']}\")\n","    print(f\"  Balanced Accuracy: {df.loc[df['balanced_accuracy'].idxmax(), 'config_name']}\")\n","    print(f\"  Precision:         {df.loc[df['precision'].idxmax(), 'config_name']}\")\n","    print(f\"  Recall:            {df.loc[df['recall'].idxmax(), 'config_name']}\")\n","    print(f\"  F1 Score:          {df.loc[df['f1'].idxmax(), 'config_name']}\")\n","    print(f\"  AUROC:             {df.loc[df['auroc'].idxmax(), 'config_name']}\")\n","    print(f\"  AUPRC:             {df.loc[df['auprc'].idxmax(), 'config_name']}\")\n","\n","folds_dir = \"/content/folds\"\n","dict_path = \"/content/lncrna-pseudo_dictionary_pooled_embeddings_RNAFM.p\"\n","\n","results_df, all_results = run_hyperparameter_tuning(folds_dir, dict_path)\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"HYPERPARAMETER TUNING COMPLETE!\")\n","print(\"=\"*60)\n","print(\"\\nResults Summary:\")\n","print(results_df[['config_name', 'accuracy', 'balanced_accuracy', 'precision',\n","                      'recall', 'f1', 'auroc', 'auprc']].to_string(index=False))"],"metadata":{"id":"Ayo_E4C0O28t","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4c204304-60bc-4da5-f430-1f64547e57f3","executionInfo":{"status":"ok","timestamp":1765265869613,"user_tz":-60,"elapsed":83418,"user":{"displayName":"Valentina Debbia","userId":"09021616900675101898"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded embeddings: 262 sequences\n","Loaded fold data from /content/folds/fold_0.pkl\n","\n","Data shapes:\n","  Train: (15268, 2560), Val: (3818, 2560), Test: (6205, 2560)\n","\n","============================================================\n","Training Config: Baseline\n","============================================================\n","Model parameters: 5,772,289\n","Epoch 1/50 | Train Loss: 0.339980 | Val Loss: 0.138299 | LR: 2.50e-04\n","Epoch 2/50 | Train Loss: 0.138897 | Val Loss: 0.113758 | LR: 3.75e-04\n","Epoch 3/50 | Train Loss: 0.112389 | Val Loss: 0.110320 | LR: 5.00e-04\n","Epoch 4/50 | Train Loss: 0.110109 | Val Loss: 0.105497 | LR: 5.00e-04\n","Epoch 5/50 | Train Loss: 0.087465 | Val Loss: 0.126174 | LR: 4.99e-04\n","Epoch 6/50 | Train Loss: 0.081738 | Val Loss: 0.144462 | LR: 4.98e-04\n","Epoch 7/50 | Train Loss: 0.059249 | Val Loss: 0.139402 | LR: 4.95e-04\n","Epoch 8/50 | Train Loss: 0.052637 | Val Loss: 0.159912 | LR: 4.91e-04\n","Epoch 9/50 | Train Loss: 0.044964 | Val Loss: 0.166289 | LR: 4.86e-04\n","Epoch 10/50 | Train Loss: 0.056304 | Val Loss: 0.158880 | LR: 4.79e-04\n","Epoch 11/50 | Train Loss: 0.040926 | Val Loss: 0.148454 | LR: 4.72e-04\n","Epoch 12/50 | Train Loss: 0.027908 | Val Loss: 0.215189 | LR: 4.64e-04\n","Epoch 13/50 | Train Loss: 0.029037 | Val Loss: 0.291485 | LR: 4.54e-04\n","Epoch 14/50 | Train Loss: 0.046487 | Val Loss: 0.165123 | LR: 4.44e-04\n","Early stopping at epoch 14\n","Optimal threshold (val F1): 0.920\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2129605062.py:344: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  box = plt.boxplot([all_test_probs[all_test_labels == 0],\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","Results for Baseline:\n","  Accuracy:          0.9892\n","  Balanced Accuracy: 0.9701\n","  Precision:         0.9785\n","  Recall:            0.9435\n","  F1 Score:          0.9607\n","  AUROC:             0.9930\n","  AUPRC:             0.9721\n","  Plots saved to:    hyperparameter_tuning_results/plots_Baseline\n","============================================================\n","\n","\n","============================================================\n","Training Config: Deeper_HighDropout\n","============================================================\n","Model parameters: 7,871,489\n","Epoch 1/50 | Train Loss: 0.524994 | Val Loss: 0.279716 | LR: 1.20e-04\n","Epoch 2/50 | Train Loss: 0.277121 | Val Loss: 0.143919 | LR: 1.80e-04\n","Epoch 3/50 | Train Loss: 0.131912 | Val Loss: 0.119006 | LR: 2.40e-04\n","Epoch 4/50 | Train Loss: 0.114730 | Val Loss: 0.111811 | LR: 3.00e-04\n","Epoch 5/50 | Train Loss: 0.103465 | Val Loss: 0.128974 | LR: 3.00e-04\n","Epoch 6/50 | Train Loss: 0.087116 | Val Loss: 0.107416 | LR: 3.00e-04\n","Epoch 7/50 | Train Loss: 0.079163 | Val Loss: 0.111132 | LR: 2.99e-04\n","Epoch 8/50 | Train Loss: 0.065700 | Val Loss: 0.121760 | LR: 2.97e-04\n","Epoch 9/50 | Train Loss: 0.071145 | Val Loss: 0.122885 | LR: 2.94e-04\n","Epoch 10/50 | Train Loss: 0.053153 | Val Loss: 0.107912 | LR: 2.91e-04\n","Epoch 11/50 | Train Loss: 0.048665 | Val Loss: 0.135771 | LR: 2.87e-04\n","Epoch 12/50 | Train Loss: 0.042369 | Val Loss: 0.150556 | LR: 2.82e-04\n","Epoch 13/50 | Train Loss: 0.041132 | Val Loss: 0.170548 | LR: 2.77e-04\n","Epoch 14/50 | Train Loss: 0.053148 | Val Loss: 0.136444 | LR: 2.71e-04\n","Epoch 15/50 | Train Loss: 0.037200 | Val Loss: 0.180790 | LR: 2.65e-04\n","Epoch 16/50 | Train Loss: 0.028020 | Val Loss: 0.147397 | LR: 2.58e-04\n","Early stopping at epoch 16\n","Optimal threshold (val F1): 0.950\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2129605062.py:344: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  box = plt.boxplot([all_test_probs[all_test_labels == 0],\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","Results for Deeper_HighDropout:\n","  Accuracy:          0.9871\n","  Balanced Accuracy: 0.9650\n","  Precision:         0.9724\n","  Recall:            0.9343\n","  F1 Score:          0.9530\n","  AUROC:             0.9941\n","  AUPRC:             0.9778\n","  Plots saved to:    hyperparameter_tuning_results/plots_Deeper_HighDropout\n","============================================================\n","\n","\n","============================================================\n","Training Config: Wider_LowLR\n","============================================================\n","Model parameters: 17,836,033\n","Epoch 1/50 | Train Loss: 0.554338 | Val Loss: 0.287642 | LR: 3.33e-05\n","Epoch 2/50 | Train Loss: 0.238479 | Val Loss: 0.134599 | LR: 5.00e-05\n","Epoch 3/50 | Train Loss: 0.130047 | Val Loss: 0.122571 | LR: 6.67e-05\n","Epoch 4/50 | Train Loss: 0.104104 | Val Loss: 0.105530 | LR: 8.33e-05\n","Epoch 5/50 | Train Loss: 0.088998 | Val Loss: 0.101837 | LR: 1.00e-04\n","Epoch 6/50 | Train Loss: 0.075455 | Val Loss: 0.107226 | LR: 1.00e-04\n","Epoch 7/50 | Train Loss: 0.059765 | Val Loss: 0.106129 | LR: 9.99e-05\n","Epoch 8/50 | Train Loss: 0.049783 | Val Loss: 0.093430 | LR: 9.95e-05\n","Epoch 9/50 | Train Loss: 0.038357 | Val Loss: 0.137501 | LR: 9.89e-05\n","Epoch 10/50 | Train Loss: 0.036079 | Val Loss: 0.109722 | LR: 9.80e-05\n","Epoch 11/50 | Train Loss: 0.025979 | Val Loss: 0.125080 | LR: 9.68e-05\n","Epoch 12/50 | Train Loss: 0.020829 | Val Loss: 0.125002 | LR: 9.55e-05\n","Epoch 13/50 | Train Loss: 0.028535 | Val Loss: 0.137678 | LR: 9.39e-05\n","Epoch 14/50 | Train Loss: 0.026975 | Val Loss: 0.134228 | LR: 9.21e-05\n","Epoch 15/50 | Train Loss: 0.013581 | Val Loss: 0.153838 | LR: 9.00e-05\n","Epoch 16/50 | Train Loss: 0.013112 | Val Loss: 0.165702 | LR: 8.78e-05\n","Epoch 17/50 | Train Loss: 0.012791 | Val Loss: 0.160611 | LR: 8.54e-05\n","Epoch 18/50 | Train Loss: 0.012027 | Val Loss: 0.202021 | LR: 8.27e-05\n","Early stopping at epoch 18\n","Optimal threshold (val F1): 0.980\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2129605062.py:344: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  box = plt.boxplot([all_test_probs[all_test_labels == 0],\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","Results for Wider_LowLR:\n","  Accuracy:          0.9908\n","  Balanced Accuracy: 0.9744\n","  Precision:         0.9822\n","  Recall:            0.9516\n","  F1 Score:          0.9666\n","  AUROC:             0.9923\n","  AUPRC:             0.9694\n","  Plots saved to:    hyperparameter_tuning_results/plots_Wider_LowLR\n","============================================================\n","\n","\n","============================================================\n","Training Config: Smaller_HighLR\n","============================================================\n","Model parameters: 1,837,057\n","Epoch 1/50 | Train Loss: 0.212075 | Val Loss: 0.124576 | LR: 6.67e-04\n","Epoch 2/50 | Train Loss: 0.125948 | Val Loss: 0.119068 | LR: 1.00e-03\n","Epoch 3/50 | Train Loss: 0.108117 | Val Loss: 0.126852 | LR: 1.00e-03\n","Epoch 4/50 | Train Loss: 0.094235 | Val Loss: 0.110742 | LR: 9.99e-04\n","Epoch 5/50 | Train Loss: 0.070736 | Val Loss: 0.175592 | LR: 9.96e-04\n","Epoch 6/50 | Train Loss: 0.065563 | Val Loss: 0.167913 | LR: 9.90e-04\n","Epoch 7/50 | Train Loss: 0.053215 | Val Loss: 0.144298 | LR: 9.82e-04\n","Epoch 8/50 | Train Loss: 0.043787 | Val Loss: 0.152323 | LR: 9.72e-04\n","Epoch 9/50 | Train Loss: 0.043918 | Val Loss: 0.156572 | LR: 9.60e-04\n","Epoch 10/50 | Train Loss: 0.034921 | Val Loss: 0.159097 | LR: 9.46e-04\n","Epoch 11/50 | Train Loss: 0.032400 | Val Loss: 0.133065 | LR: 9.30e-04\n","Epoch 12/50 | Train Loss: 0.023828 | Val Loss: 0.250212 | LR: 9.12e-04\n","Epoch 13/50 | Train Loss: 0.027691 | Val Loss: 0.246975 | LR: 8.92e-04\n","Epoch 14/50 | Train Loss: 0.019325 | Val Loss: 0.209614 | LR: 8.71e-04\n","Early stopping at epoch 14\n","Optimal threshold (val F1): 0.930\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2129605062.py:344: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  box = plt.boxplot([all_test_probs[all_test_labels == 0],\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","Results for Smaller_HighLR:\n","  Accuracy:          0.9868\n","  Balanced Accuracy: 0.9696\n","  Precision:         0.9591\n","  Recall:            0.9459\n","  F1 Score:          0.9524\n","  AUROC:             0.9914\n","  AUPRC:             0.9744\n","  Plots saved to:    hyperparameter_tuning_results/plots_Smaller_HighLR\n","============================================================\n","\n","\n","============================================================\n","Training Config: Balanced\n","============================================================\n","Model parameters: 13,378,561\n","Epoch 1/50 | Train Loss: 0.457891 | Val Loss: 0.198632 | LR: 8.00e-05\n","Epoch 2/50 | Train Loss: 0.162637 | Val Loss: 0.117979 | LR: 1.20e-04\n","Epoch 3/50 | Train Loss: 0.113791 | Val Loss: 0.105756 | LR: 1.60e-04\n","Epoch 4/50 | Train Loss: 0.098777 | Val Loss: 0.106591 | LR: 2.00e-04\n","Epoch 5/50 | Train Loss: 0.091797 | Val Loss: 0.123597 | LR: 2.00e-04\n","Epoch 6/50 | Train Loss: 0.076269 | Val Loss: 0.117209 | LR: 2.00e-04\n","Epoch 7/50 | Train Loss: 0.062210 | Val Loss: 0.132459 | LR: 1.99e-04\n","Epoch 8/50 | Train Loss: 0.056495 | Val Loss: 0.100896 | LR: 1.98e-04\n","Epoch 9/50 | Train Loss: 0.046083 | Val Loss: 0.110896 | LR: 1.96e-04\n","Epoch 10/50 | Train Loss: 0.043932 | Val Loss: 0.116466 | LR: 1.94e-04\n","Epoch 11/50 | Train Loss: 0.032404 | Val Loss: 0.133280 | LR: 1.91e-04\n","Epoch 12/50 | Train Loss: 0.031833 | Val Loss: 0.124896 | LR: 1.88e-04\n","Epoch 13/50 | Train Loss: 0.027715 | Val Loss: 0.169665 | LR: 1.85e-04\n","Epoch 14/50 | Train Loss: 0.024921 | Val Loss: 0.130949 | LR: 1.81e-04\n","Epoch 15/50 | Train Loss: 0.024947 | Val Loss: 0.143029 | LR: 1.77e-04\n","Epoch 16/50 | Train Loss: 0.017838 | Val Loss: 0.157985 | LR: 1.72e-04\n","Epoch 17/50 | Train Loss: 0.014041 | Val Loss: 0.244167 | LR: 1.67e-04\n","Epoch 18/50 | Train Loss: 0.016843 | Val Loss: 0.235500 | LR: 1.62e-04\n","Early stopping at epoch 18\n","Optimal threshold (val F1): 0.970\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2129605062.py:344: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  box = plt.boxplot([all_test_probs[all_test_labels == 0],\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","Results for Balanced:\n","  Accuracy:          0.9908\n","  Balanced Accuracy: 0.9725\n","  Precision:         0.9868\n","  Recall:            0.9470\n","  F1 Score:          0.9665\n","  AUROC:             0.9935\n","  AUPRC:             0.9783\n","  Plots saved to:    hyperparameter_tuning_results/plots_Balanced\n","============================================================\n","\n","\n","Plots saved to hyperparameter_tuning_results\n","\n","Best configurations:\n","  Accuracy:          Wider_LowLR\n","  Balanced Accuracy: Wider_LowLR\n","  Precision:         Balanced\n","  Recall:            Wider_LowLR\n","  F1 Score:          Wider_LowLR\n","  AUROC:             Deeper_HighDropout\n","  AUPRC:             Balanced\n","\n","============================================================\n","HYPERPARAMETER TUNING COMPLETE!\n","============================================================\n","\n","Results Summary:\n","       config_name  accuracy  balanced_accuracy  precision   recall       f1    auroc    auprc\n","          Baseline  0.989202           0.970088   0.978495 0.943548 0.960704 0.993013 0.972139\n","Deeper_HighDropout  0.987107           0.965011   0.972422 0.934332 0.952996 0.994085 0.977763\n","       Wider_LowLR  0.990814           0.974401   0.982164 0.951613 0.966647 0.992320 0.969384\n","    Smaller_HighLR  0.986785           0.969647   0.959112 0.945853 0.952436 0.991366 0.974362\n","          Balanced  0.990814           0.972472   0.986795 0.947005 0.966490 0.993548 0.978292\n"]}]},{"cell_type":"markdown","source":["optional"],"metadata":{"id":"RZRhieVDqoWW"}},{"cell_type":"code","source":["import os\n","import pickle\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.manifold import TSNE\n","from scipy.stats import gaussian_kde\n","import umap\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.colors import LinearSegmentedColormap\n","\n","# ==========================================\n","# MODEL DEFINITION\n","# ==========================================\n","class InteractionNN(nn.Module):\n","    \"\"\"Neural network model with embedding extraction capability\"\"\"\n","    def __init__(self, input_dim=2560, hidden_dim=1024, num_layers=4, dropout=0.2):\n","        super(InteractionNN, self).__init__()\n","        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(p=dropout)]\n","        for _ in range(num_layers - 1):\n","            layers.append(nn.Linear(hidden_dim, hidden_dim))\n","            layers.append(nn.ReLU())\n","            layers.append(nn.Dropout(p=dropout))\n","        self.hidden_layers = nn.Sequential(*layers)\n","        self.output_layer = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, x):\n","        x = self.hidden_layers(x)\n","        x = self.output_layer(x)\n","        return torch.sigmoid(x).view(-1)\n","\n","    def get_embeddings(self, x):\n","        \"\"\"Extract embeddings from second-to-last layer\"\"\"\n","        with torch.no_grad():\n","            embeddings = self.hidden_layers(x)\n","        return embeddings\n","\n","# ==========================================\n","# EMBEDDING EXTRACTION\n","# ==========================================\n","def extract_embeddings(model, data_loader, device='cuda'):\n","    \"\"\"Extract embeddings, labels, and predictions from model\"\"\"\n","    model.eval()\n","    all_embeddings = []\n","    all_labels = []\n","    all_predictions = []\n","\n","    with torch.no_grad():\n","        for batch_x, batch_y in data_loader:\n","            batch_x = batch_x.to(device)\n","            embeddings = model.get_embeddings(batch_x)\n","            all_embeddings.append(embeddings.cpu().numpy())\n","            outputs = model(batch_x)\n","            all_predictions.append(outputs.cpu().numpy())\n","            all_labels.append(batch_y.numpy())\n","\n","    embeddings = np.vstack(all_embeddings)\n","    labels = np.concatenate(all_labels)\n","    predictions = np.concatenate(all_predictions)\n","\n","    return embeddings, labels, predictions\n","\n","# ==========================================\n","# DIMENSIONALITY REDUCTION\n","# ==========================================\n","def apply_umap(embeddings, n_neighbors=15, min_dist=0.1, random_state=42):\n","    \"\"\"Apply UMAP dimensionality reduction\"\"\"\n","    print(f\"Applying UMAP (n_neighbors={n_neighbors}, min_dist={min_dist})...\")\n","    reducer = umap.UMAP(\n","        n_neighbors=n_neighbors,\n","        min_dist=min_dist,\n","        metric='euclidean',\n","        random_state=random_state,\n","        n_components=2\n","    )\n","    reduced = reducer.fit_transform(embeddings)\n","    print(f\"UMAP complete. Shape: {reduced.shape}\")\n","    return reduced\n","\n","def apply_tsne(embeddings, perplexity=30, n_iter=1000, random_state=42):\n","    \"\"\"Apply t-SNE dimensionality reduction\"\"\"\n","    print(f\"Applying t-SNE (perplexity={perplexity}, n_iter={n_iter})...\")\n","    reducer = TSNE(\n","        n_components=2,\n","        perplexity=perplexity,\n","        n_iter=n_iter,\n","        random_state=random_state\n","    )\n","    reduced = reducer.fit_transform(embeddings)\n","    print(f\"t-SNE complete. Shape: {reduced.shape}\")\n","    return reduced\n","\n","# ==========================================\n","# INDIVIDUAL PLOT FUNCTIONS\n","# ==========================================\n","def plot_true_labels(reduced_embeddings, labels, method_name, save_path):\n","    \"\"\"Create plot colored by true labels\"\"\"\n","    fig, ax = plt.subplots(figsize=(10, 8))\n","\n","    color_positive = '#0e1bcc'\n","    color_negative = '#c370ff'\n","\n","    # Plot negative samples\n","    mask_neg = labels == 0\n","    ax.scatter(\n","        reduced_embeddings[mask_neg, 0],\n","        reduced_embeddings[mask_neg, 1],\n","        c=color_negative,\n","        label='Negative',\n","        alpha=0.6,\n","        s=30,\n","        edgecolors='none'\n","    )\n","\n","    # Plot positive samples\n","    mask_pos = labels == 1\n","    ax.scatter(\n","        reduced_embeddings[mask_pos, 0],\n","        reduced_embeddings[mask_pos, 1],\n","        c=color_positive,\n","        label='Positive',\n","        alpha=0.6,\n","        s=30,\n","        edgecolors='none'\n","    )\n","\n","    ax.set_xlabel(f'{method_name} 1')\n","    ax.set_ylabel(f'{method_name} 2')\n","    ax.set_title(f'{method_name} - True Labels')\n","    ax.legend(fontsize=12, markerscale=1.5)\n","    ax.grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n","    plt.close()\n","    print(f\"Saved: {save_path}\")\n","\n","\n","def plot_prediction_confidence(reduced_embeddings, predictions, method_name, save_path):\n","    \"\"\"Create plot colored by prediction confidence\"\"\"\n","    plasma = plt.cm.get_cmap(\"plasma\")\n","    colors = plasma(np.linspace(0, 0.8, 256))\n","    plasma_no_yellow = LinearSegmentedColormap.from_list(\n","        \"plasma_no_yellow\", colors\n","        )\n","    plasma_no_yellow_r = plasma_no_yellow.reversed()\n","    fig, ax = plt.subplots(figsize=(10, 8))\n","    scatter = ax.scatter(\n","        reduced_embeddings[:, 0],\n","        reduced_embeddings[:, 1],\n","        c=predictions,\n","        cmap=plasma_no_yellow_r,\n","        alpha=0.6,\n","        s=30,\n","        edgecolors='none',\n","        vmin=0,\n","        vmax=1\n","        )\n","    cbar = plt.colorbar(scatter, ax=ax)\n","    cbar.set_label('Prediction Probability')\n","    cbar.ax.tick_params(labelsize=10)\n","\n","    ax.set_xlabel(f'{method_name} 1')\n","    ax.set_ylabel(f'{method_name} 2')\n","    ax.set_title(f'{method_name} - Prediction Confidence')\n","    ax.grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n","    plt.close()\n","    print(f\"Saved: {save_path}\")\n","\n","def plot_density(reduced_embeddings, method_name, save_path):\n","    \"\"\"Create density plot\"\"\"\n","    fig, ax = plt.subplots(figsize=(10, 8))\n","\n","    # Calculate density using Gaussian KDE\n","    xy = reduced_embeddings.T\n","    try:\n","        z = gaussian_kde(xy)(xy)\n","    except:\n","        # If KDE fails (e.g., singular matrix), use simple density\n","        print(\"KDE failed, using uniform density\")\n","        z = np.ones(reduced_embeddings.shape[0])\n","\n","    scatter = ax.scatter(\n","        reduced_embeddings[:, 0],\n","        reduced_embeddings[:, 1],\n","        c=z,\n","        cmap='viridis',\n","        alpha=0.6,\n","        s=30,\n","        edgecolors='none'\n","    )\n","\n","    cbar = plt.colorbar(scatter, ax=ax)\n","    cbar.set_label('Density')\n","    cbar.ax.tick_params(labelsize=10)\n","\n","    ax.set_xlabel(f'{method_name} 1')\n","    ax.set_ylabel(f'{method_name} 2')\n","    ax.set_title(f'{method_name} - Density Map')\n","    ax.grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n","    plt.close()\n","    print(f\"Saved: {save_path}\")\n","\n","# ==========================================\n","# MAIN VISUALIZATION PIPELINE\n","# ==========================================\n","def visualize_embeddings_separately(\n","    model_path,\n","    fold_data_path,\n","    embeddings_dict_path,\n","    output_dir='embedding_visualizations_separate',\n","    device='cuda',\n","    batch_size=512,\n","    # UMAP parameters\n","    umap_n_neighbors=15,\n","    umap_min_dist=0.1,\n","    # t-SNE parameters\n","    tsne_perplexity=30,\n","    tsne_n_iter=1000\n","):\n","    \"\"\"\n","    Complete pipeline to extract and visualize embeddings with separate plots\n","\n","    Creates 6 individual plots:\n","    - UMAP: true labels, prediction confidence, density\n","    - t-SNE: true labels, prediction confidence, density\n","    \"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Load data\n","    print(\"=\"*60)\n","    print(\"LOADING DATA\")\n","    print(\"=\"*60)\n","\n","    with open(embeddings_dict_path, 'rb') as f:\n","        embeddings_dict = pickle.load(f)\n","    print(f\"Loaded embeddings dictionary\")\n","\n","    with open(fold_data_path, 'rb') as f:\n","        fold_data = pickle.load(f)\n","    print(f\"Loaded fold data\")\n","\n","    # Prepare test data\n","    def pairs_to_arrays(pairs_list, embeddings_dict, label):\n","        X_list = []\n","        y_list = []\n","        missing = 0\n","        for (s1, s2), (t1, t2) in pairs_list:\n","            if s1 not in embeddings_dict or s2 not in embeddings_dict:\n","                missing += 1\n","                continue\n","            e1 = np.asarray(embeddings_dict[s1], dtype=np.float32)\n","            e2 = np.asarray(embeddings_dict[s2], dtype=np.float32)\n","            e1s = e1 if e1.shape[0] >= 1280 else np.pad(e1, (1280 - e1.shape[0], 0), mode='constant')\n","            e2s = e2 if e2.shape[0] >= 1280 else np.pad(e2, (1280 - e2.shape[0], 0), mode='constant')\n","            X_list.append(np.concatenate([e1s, e2s], axis=0))\n","            y_list.append(int(label))\n","        if missing:\n","            print(f\"  Skipped {missing} pairs due to missing embeddings\")\n","        return np.stack(X_list), np.array(y_list, dtype=np.int64)\n","\n","    print(\"\\nPreparing test data...\")\n","    test_pos = fold_data['test']['positives']\n","    test_neg = fold_data['test']['negatives']\n","\n","    Xp_test, yp_test = pairs_to_arrays(test_pos, embeddings_dict, label=1)\n","    Xn_test, yn_test = pairs_to_arrays(test_neg, embeddings_dict, label=0)\n","    X_test = np.concatenate([Xp_test, Xn_test], axis=0)\n","    y_test = np.concatenate([yp_test, yn_test], axis=0)\n","\n","    # Standardize\n","    scaler = StandardScaler()\n","    X_test = scaler.fit_transform(X_test)\n","\n","    print(f\"Test data shape: {X_test.shape}\")\n","    print(f\"Positive samples: {np.sum(y_test == 1)}\")\n","    print(f\"Negative samples: {np.sum(y_test == 0)}\")\n","\n","    # Create DataLoader\n","    test_dataset = TensorDataset(\n","        torch.from_numpy(X_test).float(),\n","        torch.from_numpy(y_test)\n","    )\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Load model\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"LOADING MODEL\")\n","    print(\"=\"*60)\n","    model = InteractionNN(input_dim=2560, hidden_dim=1024, num_layers=4, dropout=0.2)\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model = model.to(device)\n","    model.eval()\n","    print(f\"Model loaded from: {model_path}\")\n","\n","    # Extract embeddings\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"EXTRACTING EMBEDDINGS\")\n","    print(\"=\"*60)\n","    embeddings, labels, predictions = extract_embeddings(model, test_loader, device)\n","    print(f\"Extracted embeddings shape: {embeddings.shape}\")\n","\n","    # Save raw embeddings\n","    np.savez(\n","        os.path.join(output_dir, 'raw_embeddings.npz'),\n","        embeddings=embeddings,\n","        labels=labels,\n","        predictions=predictions\n","    )\n","    print(f\"Saved raw embeddings\")\n","\n","    # Apply UMAP and create plots\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"UMAP VISUALIZATION\")\n","    print(\"=\"*60)\n","    umap_embeddings = apply_umap(\n","        embeddings,\n","        n_neighbors=umap_n_neighbors,\n","        min_dist=umap_min_dist\n","    )\n","\n","    print(\"\\nCreating UMAP plots...\")\n","    plot_true_labels(\n","        umap_embeddings, labels, 'UMAP',\n","        os.path.join(output_dir, 'umap_true_labels.pdf')\n","    )\n","    plot_prediction_confidence(\n","        umap_embeddings, predictions, 'UMAP',\n","        os.path.join(output_dir, 'umap_prediction_confidence.pdf')\n","    )\n","    plot_density(\n","        umap_embeddings, 'UMAP',\n","        os.path.join(output_dir, 'umap_density.pdf')\n","    )\n","\n","    # Apply t-SNE and create plots\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"t-SNE VISUALIZATION\")\n","    print(\"=\"*60)\n","    tsne_embeddings = apply_tsne(\n","        embeddings,\n","        perplexity=tsne_perplexity,\n","        n_iter=tsne_n_iter\n","    )\n","\n","    print(\"\\nCreating t-SNE plots...\")\n","    plot_true_labels(\n","        tsne_embeddings, labels, 't-SNE',\n","        os.path.join(output_dir, 'tsne_true_labels.pdf')\n","    )\n","    plot_prediction_confidence(\n","        tsne_embeddings, predictions, 't-SNE',\n","        os.path.join(output_dir, 'tsne_prediction_confidence.pdf')\n","    )\n","    plot_density(\n","        tsne_embeddings, 't-SNE',\n","        os.path.join(output_dir, 'tsne_density.pdf')\n","    )\n","\n","    # Save reduced embeddings\n","    np.savez(\n","        os.path.join(output_dir, 'reduced_embeddings.npz'),\n","        umap=umap_embeddings,\n","        tsne=tsne_embeddings,\n","        labels=labels,\n","        predictions=predictions\n","    )\n","\n","    # Summary\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"VISUALIZATION COMPLETE!\")\n","    print(\"=\"*60)\n","    print(f\"\\nAll visualizations saved to: {output_dir}/\")\n","    print(\"\\nUMAP plots:\")\n","    print(\"  - umap_true_labels.pdf\")\n","    print(\"  - umap_prediction_confidence.pdf\")\n","    print(\"  - umap_density.pdf\")\n","    print(\"\\nt-SNE plots:\")\n","    print(\"  - tsne_true_labels.pdf\")\n","    print(\"  - tsne_prediction_confidence.pdf\")\n","    print(\"  - tsne_density.pdf\")\n","    print(\"\\nData files:\")\n","    print(\"  - raw_embeddings.npz\")\n","    print(\"  - reduced_embeddings.npz\")\n","\n","# ==========================================\n","# EXAMPLE USAGE\n","# ==========================================\n","if __name__ == \"__main__\":\n","    # PATHS - MODIFY THESE FOR YOUR SETUP\n","    model_path = \"hyperparameter_tuning_results/model_Baseline.pth\"\n","    fold_data_path = \"/content/folds/fold_0.pkl\"\n","    embeddings_dict_path = \"/content/lncrna-pseudo_dictionary_pooled_embeddings_RNAFM.p\"\n","    output_dir = \"embedding_visualizations_separate\"\n","\n","    # Check device\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    print(f\"Using device: {device}\\n\")\n","\n","    # Run visualization\n","    visualize_embeddings_separately(\n","        model_path=model_path,\n","        fold_data_path=fold_data_path,\n","        embeddings_dict_path=embeddings_dict_path,\n","        output_dir=output_dir,\n","        device=device,\n","        batch_size=512,\n","        # UMAP parameters\n","        umap_n_neighbors=15,\n","        umap_min_dist=0.1,\n","        # t-SNE parameters\n","        tsne_perplexity=30,\n","        tsne_n_iter=1000\n","    )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TxVo6F8gRFU7","executionInfo":{"status":"ok","timestamp":1765266004193,"user_tz":-60,"elapsed":109632,"user":{"displayName":"Valentina Debbia","userId":"09021616900675101898"}},"outputId":"f72ae2fb-92a9-4db0-94f4-4fe0fe6d3f87"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","============================================================\n","LOADING DATA\n","============================================================\n","Loaded embeddings dictionary\n","Loaded fold data\n","\n","Preparing test data...\n","Test data shape: (6205, 2560)\n","Positive samples: 868\n","Negative samples: 5337\n","\n","============================================================\n","LOADING MODEL\n","============================================================\n","Model loaded from: hyperparameter_tuning_results/model_Baseline.pth\n","\n","============================================================\n","EXTRACTING EMBEDDINGS\n","============================================================\n","Extracted embeddings shape: (6205, 1024)\n","Saved raw embeddings\n","\n","============================================================\n","UMAP VISUALIZATION\n","============================================================\n","Applying UMAP (n_neighbors=15, min_dist=0.1)...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n","  warn(\n","/usr/local/lib/python3.12/dist-packages/sklearn/manifold/_spectral_embedding.py:329: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["UMAP complete. Shape: (6205, 2)\n","\n","Creating UMAP plots...\n","Saved: embedding_visualizations_separate/umap_true_labels.pdf\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2522777202.py:144: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n","  plasma = plt.cm.get_cmap(\"plasma\")\n"]},{"output_type":"stream","name":"stdout","text":["Saved: embedding_visualizations_separate/umap_prediction_confidence.pdf\n","Saved: embedding_visualizations_separate/umap_density.pdf\n","\n","============================================================\n","t-SNE VISUALIZATION\n","============================================================\n","Applying t-SNE (perplexity=30, n_iter=1000)...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["t-SNE complete. Shape: (6205, 2)\n","\n","Creating t-SNE plots...\n","Saved: embedding_visualizations_separate/tsne_true_labels.pdf\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2522777202.py:144: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n","  plasma = plt.cm.get_cmap(\"plasma\")\n"]},{"output_type":"stream","name":"stdout","text":["Saved: embedding_visualizations_separate/tsne_prediction_confidence.pdf\n","Saved: embedding_visualizations_separate/tsne_density.pdf\n","\n","============================================================\n","VISUALIZATION COMPLETE!\n","============================================================\n","\n","All visualizations saved to: embedding_visualizations_separate/\n","\n","UMAP plots:\n","  - umap_true_labels.pdf\n","  - umap_prediction_confidence.pdf\n","  - umap_density.pdf\n","\n","t-SNE plots:\n","  - tsne_true_labels.pdf\n","  - tsne_prediction_confidence.pdf\n","  - tsne_density.pdf\n","\n","Data files:\n","  - raw_embeddings.npz\n","  - reduced_embeddings.npz\n"]}]}]}
